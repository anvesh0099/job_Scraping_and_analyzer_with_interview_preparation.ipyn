# -*- coding: utf-8 -*-
"""job Scraping and analyzer with interview preparation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGgWLq4fPZ6pBYGD-eVRij87Aln_q9w5
"""

!pip install langchain-core langchain_groq langchain_community
!pip install chromadb



from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)

response = llm.invoke("What is Ganesha?")
print(response.content)

from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://www.google.com/about/careers/applications/jobs/results/139242253561275078-engineering-manager-machine-learning-translate")
page_data = loader.load().pop().page_content
print(page_data)

from langchain_core.prompts import PromptTemplate
prompt_extract = PromptTemplate.from_template("""
        ### SCRAPED TEXT FROM WEBSITE:
        {page_data}
        ### INSTRUCTION:
        The scraped text is from the career's page of a website.
        Your job is to extract the job postings and return them in JSON format containing the
        following keys: `role`, `experience`, `skills` and `description`.
        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):
        """)
chain_extract = prompt_extract | llm
res = chain_extract.invoke(input = {'page_data': page_data})
print(res.content)

from langchain_core.output_parsers import JsonOutputParser
json_parser = JsonOutputParser()
json_res = json_parser.parse(res.content)
print(json_res)

import PyPDF2

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    text = ""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
    except Exception as e:
        print(f"Error extracting text: {e}")
    return text

pdf_text = extract_text_from_pdf("/content/99220040533 _G.Sandhya.pdf") # Replace this with the path to your PDF
print(pdf_text)

import PyPDF2
import re

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    text = ""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page_num in range(len(reader.pages)):
                page = reader.pages[page_num]
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        print(f"Error extracting text: {e}")
    return text

def extract_section(text, section_name, next_section_list):
    """Extracts section by name until the next known section."""
    pattern = rf"{section_name}.*?(?=(" + '|'.join(next_section_list) + r")|$)"
    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        return match.group(0).strip()
    return f"{section_name} section not found."

pdf_path = "/content/99220040533 _G.Sandhya.pdf"  # Replace with your actual path
pdf_text = extract_text_from_pdf(pdf_path)

# Define possible section headers that come after "Skills" or "Interests"
next_sections = ['Objective', 'Experience', 'Projects', 'Education', 'Certifications', 'Languages', 'Achievements', 'Declaration']

# Extract specific sections
skills = extract_section(pdf_text, "Skills", next_sections)
interests = extract_section(pdf_text, "Interests", next_sections)

# Output the results
print("---- Skills ----")
print(skills)
print("\n---- Interests ----")
print(interests)

!pip install chromadb

import chromadb

client = chromadb.Client()
collections = client.create_collection(name="interview_1")

collections.add(
    documents = [
        "This document is about Kedarnath",
        "This document is about Maharastra"
    ],

    ids = ['id1', 'id2'],
    metadatas = [
        {"url": "https://shrikedarnathcharitabletrust.uk.gov.in/index.html"},
        {"url": "https://www.dagdushethganpati.com/"},
    ]
)

all_docs = collections.get()

all_docs

documents = collections.get(ids = ['id1'])
documents

results = collections.query(
    query_texts = ['Query is about varanasi'],
    n_results = 2
)

results

results = collections.query(
    query_texts = ['Query is about Ganpati Visarjan'],
    n_results = 2
)

results

import uuid
import chromadb

client = chromadb.PersistentClient()
collections = client.get_or_create_collection(name = "portfolio_data")

interests

import pandas as pd
df = pd.read_csv("/content/skills_and_interests.csv")
df

df

if not collections.count():
  for i, row in  df.iterrows():
    collections.add(documents = row['Skills/Interests'], ids =[str(uuid.uuid4())])

tech = collections.query(query_texts=['Experience in Python', 'React'], n_results = 2, ).get('documents', [])

tech

json_res

if type(json_res) == dict:
  job = json_res.get("skills", [])
else:
  job = json_res[0].get("skills", [])

job

prompt_skills_and_question = PromptTemplate.from_template("""
        ### JOB DESCRIPTION:
        {job_description}

        ### INSTRUCTION:
        You are Mishu Dhar Chando, the CEO of Knowledge Doctor, a YouTube channel specializing in educating individuals on machine learning, deep learning, and natural language processing.
        Your expertise lies in bridging the gap between theoretical knowledge and practical applications through engaging content and innovative problem-solving techniques.
        Your job is to:
        1. Analyze the given job description to identify the required technical skills and match them with the provided skill set to calculate a percentage match.
        2. Generate a list of relevant interview questions based on the job description.
        3. Return the information in JSON format with the following keys:
            - `skills_match`: A dictionary where each key is a skill, and the value is the matching percentage.
            - `interview_questions`: A list of tailored questions related to the job description.

        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):

        """)

chain_skills_and_question = prompt_skills_and_question | llm
res = chain_skills_and_question.invoke({"job_description": str(job)})
print(res.content)

!pip install gradio

import gradio as gr
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
import pandas as pd
import uuid
import chromadb
from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)

def preproces_job_posting(url, portfolio_csv):
  loader = WebBaseLoader(url)
  page_data = loader.load().pop().page_content
  prompt_extract = PromptTemplate.from_template("""
        ### SCRAPED TEXT FROM WEBSITE:
        {page_data}
        ### INSTRUCTION:
        The scraped text is from the career's page of a website.
        Your job is to extract the job postings and return them in JSON format containing the
        following keys: `role`, `experience`, `skills` and `description`.
        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):
        """)
  chain_extract = prompt_extract | llm
  res_1 = chain_extract.invoke(input = {'page_data': page_data})
  json_parser = JsonOutputParser()
  json_res = json_parser.parse(res_1.content)

  df = pd.read_csv(portfolio_csv)

  client = chromadb.PersistentClient('vectorstore')
  collections = client.get_or_create_collection(name="portfolio_app")
  if not collections.count():
    for i, row in  df.iterrows():
      collections.add(documents = row['Skills/Interests'], ids =[str(uuid.uuid4())])
  job  = json_res.get('skills', []) if type(json_res) == dict else json_res[0].get('skills', [])

  prompt_skills_and_question = PromptTemplate.from_template("""
        ### JOB DESCRIPTION:
        {job_description}

        ### INSTRUCTION:
        You are Mishu Dhar Chando, the CEO of Knowledge Doctor, a YouTube channel specializing in educating individuals on machine learning, deep learning, and natural language processing.
        Your expertise lies in bridging the gap between theoretical knowledge and practical applications through engaging content and innovative problem-solving techniques.
        Your job is to:
        1. Analyze the given job description to identify the required technical skills and match them with the provided skill set to calculate a percentage match.
        2. Generate a list of relevant interview questions based on the job description.
        3. Return the information in JSON format with the following keys:
            - `skills_match`: A dictionary where each key is a skill, and the value is the matching percentage.
            - `interview_questions`: A list of tailored questions related to the job description.

        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):

        """)
  chain_skills_and_question = prompt_skills_and_question | llm
  res1 = chain_skills_and_question.invoke({"job_description": str(job)})
  final_result = json_parser.parse(res1.content)
  return final_result

def gradio_interface(url, portfolio_csv):
  try:
    result = preproces_job_posting(url, portfolio_csv)
    return result
  except Exception as e:
    return {"error": str(e)}

with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
  gr.Markdown("# Job Scraping & Analyzer with Interview Preparation Questions Using Gen-AI")

  with gr.Row():
    url_input = gr.Textbox(label = "Website URL", placeholder="Enter the url of the job posting")
    portfolio_input = gr.File(label = "Upload Portfolio CSV")

  analyze_button = gr.Button("Analyze Job Posting")
  output_box = gr.JSON(label = "Result")

  analyze_button.click(gradio_interface, inputs = [url_input, portfolio_input], outputs = output_box)

app.launch()

!pip install PyPDF2